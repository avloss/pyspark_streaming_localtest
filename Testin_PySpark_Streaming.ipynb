{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing PySpark Streaming Locally\n",
    "\n",
    "PySpark Streaming has been around for a few years now. It's Spark's solution for processing data near-realtime. The\n",
    "idea behind it, is to make Data Stream processing as easy as data processing. Though there are some commands specific to Spark Streaming. For instance **`updateStateByKey`**. So it can be non-straight forward to test it.\n",
    "\n",
    "Let's start with a simple example and working towards testable PySpark Streaming code.\n",
    "\n",
    "Let's create Spark Context first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[*]\")\n",
    "sc.setCheckpointDir(\"/tmp/spark_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming needs a checkpoint set to run some of it's operations, we are setting it here.\n",
    "\n",
    "Let's make sure Spark Context works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This is easy to re-run and to test. Even quite complex transformations can be tested this way on a smaller sub-set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 1),\n",
       " ('this', 1),\n",
       " ('just', 1),\n",
       " ('unfortunately', 1),\n",
       " ('another', 1),\n",
       " ('will', 2),\n",
       " ('it', 1),\n",
       " ('yes', 2),\n",
       " ('be', 2),\n",
       " ('word', 1),\n",
       " ('count', 1),\n",
       " ('example', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    sc.parallelize(\"yes this will be another word count example unfortunately yes it will be just that\".split())\n",
    "    .map(lambda x: (x,1))\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Unfortunately when it comes to Spark Streaming it doesn't seem to be as easy to achieve the same result. If we start Spark Streaming Context we won't be able to add operations to it \"as we go\". The only way I found it possible to easily Spark Streaming was using following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "BATCH_INTERVAL_SEC = 1\n",
    "SCC_TIMEOUT = 2\n",
    "\n",
    "def apply_with_spark_streaming(readings, transform):\n",
    "\n",
    "    ssc = StreamingContext(sc, BATCH_INTERVAL_SEC)\n",
    "\n",
    "    input_stream = ssc.queueStream(readings)\n",
    "\n",
    "    transformed_readings=[]\n",
    "\n",
    "    transformed_stream = transform(input_stream)\n",
    "\n",
    "    transformed_stream.foreachRDD(lambda rdd: transformed_readings.append(rdd.collect()))\n",
    "\n",
    "    ssc.start()\n",
    "\n",
    "    ssc.awaitTerminationOrTimeout(SCC_TIMEOUT)\n",
    "\n",
    "    stop_spark_context = False\n",
    "    stop_gracefully = True\n",
    "    ssc.stop(stop_spark_context, stop_gracefully)\n",
    "    \n",
    "    return transformed_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('that', 1),\n",
       "  ('this', 1),\n",
       "  ('just', 1),\n",
       "  ('unfortunately', 1),\n",
       "  ('another', 1),\n",
       "  ('will', 2),\n",
       "  ('it', 1),\n",
       "  ('yes', 2),\n",
       "  ('be', 2),\n",
       "  ('word', 1),\n",
       "  ('count', 1),\n",
       "  ('example', 1)],\n",
       " [('of', 1),\n",
       "  ('that', 1),\n",
       "  ('unfortunately', 1),\n",
       "  ('just', 1),\n",
       "  ('this', 1),\n",
       "  ('another', 2),\n",
       "  ('will', 2),\n",
       "  ('it', 1),\n",
       "  ('yes', 2),\n",
       "  ('word', 2),\n",
       "  ('be', 2),\n",
       "  ('from', 1),\n",
       "  ('batch', 1),\n",
       "  ('the', 1),\n",
       "  ('count', 2),\n",
       "  ('example', 2)],\n",
       " [('of', 1),\n",
       "  ('that', 1),\n",
       "  ('unfortunately', 1),\n",
       "  ('just', 1),\n",
       "  ('this', 1),\n",
       "  ('another', 2),\n",
       "  ('will', 2),\n",
       "  ('it', 1),\n",
       "  ('yes', 2),\n",
       "  ('word', 2),\n",
       "  ('be', 2),\n",
       "  ('from', 1),\n",
       "  ('batch', 1),\n",
       "  ('the', 1),\n",
       "  ('count', 2),\n",
       "  ('example', 2)],\n",
       " [('of', 1),\n",
       "  ('that', 1),\n",
       "  ('unfortunately', 1),\n",
       "  ('just', 1),\n",
       "  ('this', 1),\n",
       "  ('another', 2),\n",
       "  ('will', 2),\n",
       "  ('it', 1),\n",
       "  ('yes', 2),\n",
       "  ('word', 2),\n",
       "  ('be', 2),\n",
       "  ('from', 1),\n",
       "  ('batch', 1),\n",
       "  ('the', 1),\n",
       "  ('count', 2),\n",
       "  ('example', 2)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = \"yes this will be another word count example unfortunately yes it will be just that\"\n",
    "batch2 = \"another batch from the example of word count\"\n",
    "\n",
    "def transform(dstream):\n",
    "    return (dstream\n",
    "           .map(lambda x: (x,1))\n",
    "           .updateStateByKey(updateFunction)\n",
    "           )\n",
    "\n",
    "# taken from https://spark.apache.org/docs/latest/streaming-programming-guide.html#updatestatebykey-operation\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "\n",
    "apply_with_spark_streaming([batch1.split(), batch2.split()], transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So, what is happening here is that we define our `transform`, and then pass it along with a test dataset to `apply_with_spark_streaming`. Which starts Spark Streaming context and runs it for `SCC_TIMEOUT` seconds. after that we are shutting down Spart Streaming context, but not the Spark Context itself, this was we can re-run this function without restarting this notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:lj]",
   "language": "python",
   "name": "conda-env-lj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
